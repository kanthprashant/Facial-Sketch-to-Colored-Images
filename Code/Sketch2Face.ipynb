{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from os import path\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import  torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking torch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ef2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17284812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing root directories\n",
    "training_dir = \"./train\"\n",
    "test_dir = \"./test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e160d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making custom pytorch dataset\n",
    "folder_dataset = dset.ImageFolder(root=training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making custom pytorch datset class\n",
    "scale = 255\n",
    "class get_Dataset(Dataset):\n",
    "    def __init__(self, imageFolderDataset):\n",
    "        self.imageFolderDataset = imageFolderDataset \n",
    "        self.len = int(len(self.imageFolderDataset.imgs)/2)\n",
    "        self.object = np.ones((self.len, 3, 256, 256))\n",
    "        self.target = np.ones((self.len, 3, 256, 256))\n",
    "        for i in range(0, self.len, 1):\n",
    "            x = cv2.resize(cv2.imread(self.imageFolderDataset.imgs[i+2270][0]), (256,256))\n",
    "            bo,go,ro = cv2.split(x)           # get b, g, r\n",
    "            rgb_imgo = cv2.merge([ro,go,bo]) \n",
    "            self.object[i] = rgb_imgo.transpose(2, 1, 0)\n",
    "            y = cv2.resize(cv2.imread(self.imageFolderDataset.imgs[i][0]), (256,256))\n",
    "            bp,gp,rp = cv2.split(y)           # get b, g, r\n",
    "            rgb_imgp = cv2.merge([rp,gp,bp]) \n",
    "            self.target[i] = rgb_imgp.transpose(2, 1, 0)\n",
    "            #print(i)\n",
    "        # Normalization between -1 to 1\n",
    "        self.object = torch.from_numpy(((self.object/(scale / 2)) -1 )).float()\n",
    "        self.target = torch.from_numpy(((self.target/(scale / 2)) -1 )).float()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.object[index], self.target[index]        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_Dataset(imageFolderDataset = folder_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset , batch_size = 32 , shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88425b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4×4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b155ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel=4,\n",
    "        strides=2,\n",
    "        padding=1,\n",
    "        activation=True,\n",
    "        batchnorm=True,\n",
    "        dropout=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.ReLU(True)\n",
    "\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - Encoder: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4×4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder/donwsample convs\n",
    "        self.encoders = [\n",
    "            DownSampleConv(3, 64, batchnorm=False),  # bs x 64 x 128 x 128\n",
    "            DownSampleConv(64, 128),  # bs x 128 x 64 x 64\n",
    "            DownSampleConv(128, 256),  # bs x 256 x 32 x 32\n",
    "            DownSampleConv(256, 512),  # bs x 512 x 16 x 16\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 8 x 8\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 4 x 4\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 2 x 2\n",
    "            DownSampleConv(512, 512, batchnorm=False),  # bs x 512 x 1 x 1\n",
    "        ]\n",
    "\n",
    "        # decoder/upsample convs\n",
    "        self.decoders = [\n",
    "            UpSampleConv(512, 512, dropout=True),  # bs x 512 x 2 x 2\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 4 x 4\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 8 x 8\n",
    "            UpSampleConv(1024, 512),  # bs x 512 x 16 x 16\n",
    "            UpSampleConv(1024, 256),  # bs x 256 x 32 x 32\n",
    "            UpSampleConv(512, 128),  # bs x 128 x 64 x 64\n",
    "            UpSampleConv(256, 64),  # bs x 64 x 128 x 128\n",
    "        ]\n",
    "        self.decoder_channels = [512, 512, 512, 512, 256, 128, 64]\n",
    "        self.final_conv = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips_cons = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "            skips_cons.append(x)\n",
    "\n",
    "        skips_cons = list(reversed(skips_cons[:-1]))\n",
    "        decoders = self.decoders[:-1]\n",
    "\n",
    "        for decoder, skip in zip(decoders, skips_cons):\n",
    "            x = decoder(x)\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat((x, skip), axis=1)\n",
    "\n",
    "        x = self.decoders[-1](x)\n",
    "        # print(x.shape)\n",
    "        x = self.final_conv(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89301bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights initializiation\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the generator\n",
    "netG1 = G().float().cuda()\n",
    "netG1.load_state_dict(torch.load(\"./models/gen_2dat_256.pt\"))\n",
    "netG2 = G().float().cuda()\n",
    "netG2.load_state_dict(torch.load(\"./models/gen_2dat_256.pt\"))\n",
    "#netG1.apply(_weights_init)\n",
    "clear_output()\n",
    "netG1.eval()\n",
    "#netG(Variable(input)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = DownSampleConv(6, 64, batchnorm=False)\n",
    "        self.d2 = DownSampleConv(64, 128)\n",
    "        self.d3 = DownSampleConv(128, 256)\n",
    "        self.d4 = DownSampleConv(256, 512)\n",
    "        self.final = nn.Conv2d(512, 1, kernel_size = 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "        x0 = self.d1(x)\n",
    "        x1 = self.d2(x0)\n",
    "        x2 = self.d3(x1)\n",
    "        x3 = self.d4(x2)\n",
    "        xn = self.final(x3)\n",
    "        xs = self.sig(xn)\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the discriminator\n",
    "netD = D().float().cuda()\n",
    "netD.load_state_dict(torch.load(\"./models/dis_2dat_256.pt\"))\n",
    "# netD.apply(_weights_init)\n",
    "clear_output()\n",
    "netD.eval()\n",
    "#netD(torch.cat((Variable(input), Variable(label)) , dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21edf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() # We create a criterion object that will measure the error between the prediction and the target.\n",
    "L1loss=nn.L1Loss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n",
    "optimizerG1 = optim.Adam(netG1.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator.\n",
    "optimizerG2 = optim.Adam(netG2.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing the previous result\n",
    "sketch_result = \"./results9/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a08720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 200\n",
    "alpha = 100\n",
    "for epoch in range(max_epoch): # We iterate over 25 epochs.\n",
    "    netG1.train()\n",
    "    netG2.train()\n",
    "    netD.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):     # We iterate over the images of the dataset.\n",
    "\n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "        netD.zero_grad() \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        obj1 , label1  =  data\n",
    "        obj = Variable(obj1.cuda())\n",
    "        label = Variable(label1.cuda())\n",
    "        #label1 = label1[:,0,:,:]\n",
    "        #label1 = label1.unsqueeze(1)\n",
    "        \n",
    "        #label1 = transforms.functional.rgb_to_grayscale(label1, 3)\n",
    "        #label1 = torch.repeat_interleave(label1[:,None,:,:], 3, dim=1)\n",
    "        #label1 = label1[:,:,0,:,:]\n",
    "        \n",
    "        #label_grey = Variable(label1.cuda())\n",
    "        #print(label1.shape)\n",
    "        \n",
    "        target = torch.ones((label.size()[0] ,1, 16 , 16)).fill_(1).float().cuda()\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "        targetv = Variable(target)\n",
    "        output = netD(obj, label) \n",
    "        # print(\"targetv.shape : \", targetv.shape )\n",
    "        # print(output.shape)\n",
    "        # print(label.shape)\n",
    "\n",
    "        errD_color = criterion(output , targetv)\n",
    "        errD_color = errD_color*0.5\n",
    "        errD_color.backward(retain_graph=True)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------        \n",
    "        fake1 = netG1(obj) \n",
    "        targetv = Variable(target.fill_(0)) \n",
    "        fake2 = netG2(fake1)\n",
    "        output = netD(obj , fake2)\n",
    "\n",
    "        errD_fake = criterion(output, targetv) \n",
    "        errD_fake = errD_fake*0.5\n",
    "        errD = (errD_color + errD_fake)\n",
    "        #errD.backward(retain_graph = True)\n",
    "        errD_fake.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------        \n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG1.zero_grad() \n",
    "        netG2.zero_grad()\n",
    "        targetv = Variable(target.fill_(1))\n",
    "        output = netD(obj, fake1) \n",
    "        #errG1 = criterion(output, targetv)\n",
    "        errG1 = criterion(output, targetv)\n",
    "        errG1_L1 = L1loss(fake1, label)\n",
    "        errG2_L1 = L1loss(fake2 , label)\n",
    "        errG = errG1  +  alpha *  errG1_L1 + alpha * errG2_L1\n",
    "        errG.backward() \n",
    "        optimizerG1.step()\n",
    "        optimizerG2.step()\n",
    "        print('[%d/%d] [%d/%d] Loss_D: %.20f Loss_G: %.20f' % (epoch, max_epoch, i, len(train_loader), errD.item(), errG.item()/alpha))\n",
    "\n",
    "        if i % 70 == 0 and i != 0:\n",
    "            temp =obj.cpu().data\n",
    "            #temp = np.transpose(temp, [0, 3, 2, 1])\n",
    "            vutils.save_image(temp , '%sepoch_%2d_%03d_object.png' % ( sketch_result ,epoch   , i) , normalize=True)\n",
    "            temp =fake1.cpu().data\n",
    "            #print(temp.shape)\n",
    "            #temp = np.transpose(temp, [0, 3, 2, 1])\n",
    "            vutils.save_image(temp , '%sepoch_%2d_%03d_fromGenerator1.png' % ( sketch_result ,epoch   , i) , normalize=True)\n",
    "            temp =fake2.cpu().data\n",
    "            #print(temp.shape)\n",
    "            #temp = np.transpose(temp, [0, 3, 2, 1])\n",
    "            vutils.save_image(temp , '%sepoch_%2d_%03d_fromGenerator2.png' % ( sketch_result ,epoch   , i) , normalize=True)\n",
    "            temp =label.cpu().data        \n",
    "            #temp = np.transpose(temp, [0, 3, 2, 1])            \n",
    "            vutils.save_image(temp , '%sepoch_%2d_%03d_target.png' % ( sketch_result ,epoch   , i) , normalize=True)\n",
    "            # saving the loss in file\n",
    "            f=open(sketch_result + \"LOG.txt\", \"a+\")\n",
    "            f.write('[%d/%d] [%d/%d] Loss_D: %.20f Loss_G: %.20f\\n' % (epoch, max_epoch, i, len(train_loader), errD.item(), errG.item()/alpha))\n",
    "            f.close()\n",
    "            clear_output()\n",
    "        \n",
    "            if epoch % 75 == 0 and epoch != 0:\n",
    "                torch.save(netD.cuda().state_dict(), \"./models/dis_s2f_\"+str(epoch)+\"_final.pt\")\n",
    "                print(\"Discriminator Saved Successfully\")\n",
    "                torch.save(netG1.cuda().state_dict(), \"./models/gen1_s2f_\"+str(epoch)+\"_final.pt\")\n",
    "                print(\"Generator Saved Successfully\")\n",
    "                torch.save(netG2.cuda().state_dict(), \"./models/gen2_s2f_\"+str(epoch)+\"_final.pt\")\n",
    "                print(\"Generator Saved Successfully\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(netD.cuda().state_dict(), \"./models/dis_s2f_final.pt\")\n",
    "print(\"Discriminator Saved Successfully\")\n",
    "torch.save(netG1.cuda().state_dict(), \"./models/gen1_s2f_final.pt\")\n",
    "print(\"Generator Saved Successfully\")\n",
    "torch.save(netG2.cuda().state_dict(), \"./models/gen2_s2f_final.pt\")\n",
    "print(\"Generator Saved Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_1 = G().float().cuda()\n",
    "generator_2 = G().float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_1.load_state_dict(torch.load(\"./models/gen1_s2f_final.pt\"))\n",
    "generator_1.eval()\n",
    "generator_2.load_state_dict(torch.load(\"./models/gen2_s2f_final.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./test/sketches/0001.jpg\"\n",
    "img = cv2.resize(cv2.imread(path), (256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,g,r = cv2.split(img)           # get b, g, r\n",
    "rgb_img = cv2.merge([r,g,b]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(rgb_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = rgb_img.transpose(2,1,0)\n",
    "rgb_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = torch.from_numpy(((rgb_img/(255 / 2)) -1 )).float()\n",
    "img_test=img_test.expand(1,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14203fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake1 = generator_1(img_test.to('cuda'))\n",
    "fake2 = generator_2(fake1.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_viz = fake2[0].cpu().detach().numpy()\n",
    "fake_viz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9319a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_viz = fake_viz.transpose(2,1,0)\n",
    "fake_viz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98197b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake_viz)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_viz = np.transpose(fake_viz, [2,0,1])\n",
    "fake_viz = torch.from_numpy(fake_viz)\n",
    "vutils.save_image(fake_viz , 'testp3.png' , normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e2591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4b9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38fd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
